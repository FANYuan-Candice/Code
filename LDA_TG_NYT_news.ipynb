{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8cd6f-32fe-46bb-9de2-b29c6836f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from pylab import rcParams    \n",
    "rcParams['figure.dpi']=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c6dbf1-ef48-47ce-bab2-0a1f1fa4f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\fanyu\\Desktop\\Jupiter\")\n",
    "df = pd.read_csv(\"WES.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f578bc0-d9c8-4744-9430-d9b1a17519af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec6b79f-ed51-4321-9372-0d49ee16dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [re.sub(r'\\([^)]*\\)', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "data = [re.sub(r'\\b(?:America|United States|U\\.S\\.|USA)\\b', 'US', sent, flags=re.IGNORECASE) for sent in data]\n",
    "data = [re.sub(r'\\bUnited Nations\\b', 'UN', sent, flags=re.IGNORECASE) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac489ee-defe-41df-82fe-650b1ce654d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "data_words = list(sent_to_words(data))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2aeee-09af-40e8-981b-dd8f4e9151ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_mod[data_words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece8aa8-d050-4efd-bfa6-bb549493a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6581ed6-e4fb-46c5-a938-eb93ca7492f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [bigram for sublist in data_lemmatized for bigram in list(ngrams(sublist, 2))]\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "top_20_bigrams = bigram_counts.most_common(20)\n",
    "\n",
    "print(\"Top 20 Bigrams:\")\n",
    "for bigram, count in top_20_bigrams:\n",
    "    print(bigram, \"-\", count)\n",
    "\n",
    "bigram, count = zip(*top_20_bigrams)\n",
    "\n",
    "plt.barh(range(len(bigram)), count, color='skyblue')\n",
    "plt.yticks(range(len(bigram)), bigram)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Bigram')\n",
    "plt.title(\"Top 20 Bigrams in TG and NYT\")\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.savefig('C:/Users/fanyu/Desktop/Jupiter/top_20_bigrams_WES.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0492ca-16ae-4c95-9303-7fb5dbd0a6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "tfidf = TfidfModel(corpus,id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818a256-d219-4c4d-b0f0-78f577c2a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "perplexity_values = []\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in range (2,21,1):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics= num_topics,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "    model_list.append(lda_model)\n",
    "    log_perplexity = lda_model.log_perplexity(corpus)\n",
    "    perplexity = math.exp(log_perplexity) \n",
    "    perplexity_values.append(perplexity)\n",
    "\n",
    "    coherencemodel = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence = coherencemodel.get_coherence()\n",
    "    coherence_values.append(round(coherence, 3))\n",
    "    print(f'Num Topics: {num_topics} - Perplexity: {perplexity}, Coherence Score: {round(coherence, 3)}')\n",
    "\n",
    "print('\\nPerplexity Values: ', perplexity_values)\n",
    "print('Coherence Values: ', coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd911c-0aa5-4ca3-92b1-1f71c5ebb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_range = list(range(2, 21))\n",
    "\n",
    "# Plot perplexity values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_topics_range, perplexity_values, marker='o')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Perplexity by Number of Topics')\n",
    "plt.xticks(num_topics_range) \n",
    "\n",
    "# Plot coherence values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_topics_range, coherence_values, marker='o')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Coherence Score by Number of Topics')\n",
    "plt.xticks(num_topics_range) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('C:/Users/fanyu/Desktop/Jupiter/Perplexity_Coherence (WES).png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15de246-1299-410b-acd7-e3811e1a2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_topics = 6  \n",
    "best_lda_model = model_list[best_num_topics - 2] \n",
    "\n",
    "num_words = 10\n",
    "topics = best_lda_model.print_topics(num_words=num_words)\n",
    "for topic in topics:\n",
    "    print(f'Topic {topic[0]}: {topic[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3927c2fd-4d2c-4378-9763-5b2bfc55166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce12bba-b6fc-44d7-92e3-ec94611bbab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from pylab import rcParams    \n",
    "rcParams['figure.dpi']=300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3758b-eeb9-49da-8b2e-9b5dbd343659",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\fanyu\\Desktop\\Jupiter\")\n",
    "data = pd.read_csv(\"all_news_cleaned.csv\",encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea4a86-681e-4b79-9e2c-987befa105f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09d01c-e133-45df-a1f8-f62d2428da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = data['cleaned_content']\n",
    "\n",
    "text_data = []\n",
    "for text in text_column:\n",
    "    sentence_list = nltk.sent_tokenize(text) \n",
    "    text_data.extend(sentence_list)\n",
    "    \n",
    "preprocessed_data = []\n",
    "for sentence in text_data:\n",
    "    preprocessed_sentence = [word.lower() for word in sentence.split() if word.isalpha()] \n",
    "    preprocessed_data.append(preprocessed_sentence)\n",
    "\n",
    "embedding_data = []\n",
    "for sentence in preprocessed_data:\n",
    "    sentence_embedding = [model.get_vector(word) for word in sentence if word in model.key_to_index]\n",
    "    if sentence_embedding:\n",
    "        embedding_data.append(sum(sentence_embedding) / len(sentence_embedding))\n",
    "    else:\n",
    "        embedding_data.append(None)\n",
    "\n",
    "np.array(embedding_data).shape\n",
    "\n",
    "embedding = pd.DataFrame(embedding_data, columns=['Dim{}'.format(i) for i in range(1, np.array(embedding_data).shape[1]+ 1)])\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "pca = sklearnPCA(n_components=2)\n",
    "pca.fit_transform(embedding)\n",
    "\n",
    "data3=pd.read_csv('all_news_cleaned.csv',index_col=0)\n",
    "data3[['Dim1','Dim2']]=pca.fit_transform(embedding) \n",
    "\n",
    "sns.scatterplot(data3,x=\"Dim1\",y=\"Dim2\",hue=\"source\")\n",
    "plt.title(\"Word Embedding (Semantic Relationship)\")\n",
    "plt.savefig('Word_embedding.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f8bc4-9769-4a91-9c23-a95a99e4d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
